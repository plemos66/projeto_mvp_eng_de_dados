{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96816ed7-b08a-4ca3-abb9-f99880c3535d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Projeto de Análise de Dados - IMDB\n",
    "\n",
    "## Objetivo do Notebook 2\n",
    "\n",
    "Neste notebook, realizei a etapa da extração e upload dos dados brutos do projeto. Os arquivos originais, no formato TSV, foram carregados para o ambiente do Databricks e estruturados como DataFrames. Também foram criadas as primeiras tabelas no Metastore para facilitar consultas posteriores via SQL.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e812a8e8-385b-4b21-86f4-58b515a2cc4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1 - Importação das bibliotecas PySpark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66ce571e-fbd2-44ec-acd5-5cac0d08c757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c68f858-ac8e-47df-9159-4885c0421d68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2 - Limpeza do ambiente antes da carga dos dados para garantir um ambiente limpo e sem resíduos de execuções anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a2129d64-2e5a-4de1-b59d-4683ebd0865b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limpeza inicial concluída ✅\n"
     ]
    }
   ],
   "source": [
    "tables = [\n",
    "    \"name_basics\", \n",
    "    \"title_akas\", \n",
    "    \"title_basics\", \n",
    "    \"title_crew\", \n",
    "    \"title_episode\", \n",
    "    \"title_ratings\",\n",
    "    \"title_principals\"\n",
    "]\n",
    "\n",
    "# Remover diretórios existentes no warehouse\n",
    "for table in tables:\n",
    "    path = f\"dbfs:/user/hive/warehouse/{table}\"\n",
    "    dbutils.fs.rm(path, True)\n",
    "\n",
    "print(\"Limpeza inicial concluída ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a38cc7a-0134-4998-9265-30588c0acfbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3 - Upload e Leitura dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "35e09bbc-e756-4352-9246-8c14c2f62df0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leitura dos dados concluída ✅\n"
     ]
    }
   ],
   "source": [
    "def ler_tsv(caminho):\n",
    "    return spark.read.option(\"header\", \"true\").option(\"sep\", \"\\t\").option(\"inferSchema\", \"true\").csv(caminho)\n",
    "\n",
    "name_basics = ler_tsv(\"/FileStore/tables/name_basics_tsv.gz\")\n",
    "title_akas = ler_tsv(\"/FileStore/tables/title_akas_tsv.gz\")\n",
    "title_basics = ler_tsv(\"/FileStore/tables/title_basics_tsv.gz\")\n",
    "title_crew = ler_tsv(\"/FileStore/tables/title_crew_tsv.gz\")\n",
    "title_episode = ler_tsv(\"/FileStore/tables/title_episode_tsv.gz\")\n",
    "title_ratings = ler_tsv(\"/FileStore/tables/title_ratings_tsv.gz\")\n",
    "title_principals = ler_tsv(\"/FileStore/tables/title_principals_tsv.gz\")\n",
    "\n",
    "print(\"Leitura dos dados concluída ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57172fb2-ab85-42a1-b41c-5ad26a4cd4c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4 - Verificações iniciais: contagem das linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c4841ab-1701-4030-8919-270dc65da405",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de registros por tabela:\nname_basics: 14322508\ntitle_akas: 51851599\ntitle_basics: 11576566\ntitle_crew: 11576566\ntitle_episode: 8906003\ntitle_ratings: 1555765\ntitle_principals: 91920801\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Quantidade de registros por tabela:\")\n",
    "print(f\"name_basics: {name_basics.count()}\")\n",
    "print(f\"title_akas: {title_akas.count()}\")\n",
    "print(f\"title_basics: {title_basics.count()}\")\n",
    "print(f\"title_crew: {title_crew.count()}\")\n",
    "print(f\"title_episode: {title_episode.count()}\")\n",
    "print(f\"title_ratings: {title_ratings.count()}\")\n",
    "print(f\"title_principals: {title_principals.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a39ea4a9-03c0-414b-a8fe-8ff0a0968c5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5 - Criação das Temp Views\n",
    "\n",
    "Após a leitura dos arquivos e verificações iniciais, criei visualizações temporárias (Temp Views), para consultar os dados via SQL nos próximos notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e18d5cc-faa5-4e73-9099-dc258a03bb68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "name_basics.createOrReplaceTempView(\"name_basics\")\n",
    "title_akas.createOrReplaceTempView(\"title_akas\")\n",
    "title_basics.createOrReplaceTempView(\"title_basics\")\n",
    "title_crew.createOrReplaceTempView(\"title_crew\")\n",
    "title_episode.createOrReplaceTempView(\"title_episode\")\n",
    "title_ratings.createOrReplaceTempView(\"title_ratings\")\n",
    "title_principals.createOrReplaceTempView(\"title_principals\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c2ae31e-3214-4202-b07d-8081df4e7414",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6 - Remoção de tabelas e diretórios antigos e criação das tabelas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc04d634-b32e-40fc-8836-b0f46ea7e0d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvamento em Parquet concluído ✅\nTabelas criadas no Metastore ✅\n"
     ]
    }
   ],
   "source": [
    "# DROP de tabelas existentes no metastore\n",
    "spark.sql(\"DROP TABLE IF EXISTS name_basics\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS title_akas\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS title_basics\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS title_crew\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS title_episode\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS title_ratings\")\n",
    "spark.sql(\"DROP TABLE IF EXISTS title_principals\")\n",
    "\n",
    "# Limpeza dos diretórios\n",
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/name_basics\", True)\n",
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/title_akas\", True)\n",
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/title_basics\", True)\n",
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/title_crew\", True)\n",
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/title_episode\", True)\n",
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/title_ratings\", True)\n",
    "dbutils.fs.rm(\"dbfs:/user/hive/warehouse/title_principals\", True)\n",
    "\n",
    "# Criação das tabelas\n",
    "name_basics.write.mode(\"overwrite\").parquet(\"/FileStore/parquet/name_basics\")\n",
    "title_akas.write.mode(\"overwrite\").parquet(\"/FileStore/parquet/title_akas\")\n",
    "title_basics.write.mode(\"overwrite\").parquet(\"/FileStore/parquet/title_basics\")\n",
    "title_crew.write.mode(\"overwrite\").parquet(\"/FileStore/parquet/title_crew\")\n",
    "title_episode.write.mode(\"overwrite\").parquet(\"/FileStore/parquet/title_episode\")\n",
    "title_ratings.write.mode(\"overwrite\").parquet(\"/FileStore/parquet/title_ratings\")\n",
    "title_principals.write.mode(\"overwrite\").parquet(\"/FileStore/parquet/title_principals\")\n",
    "\n",
    "print(\"Salvamento em Parquet concluído ✅\")\n",
    "\n",
    "# Criação das tabelas no metastore a partir dos Parquet salvos\n",
    "for table in tables:\n",
    "    spark.sql(f\"CREATE TABLE {table} USING parquet LOCATION '/FileStore/parquet/{table}'\")\n",
    "\n",
    "print(\"Tabelas criadas no Metastore ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19f94de8-064c-4a5e-9691-e4eca50efec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7 - Verificação das tabelas no metastore e seus schemas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "819677cb-f8c4-49a9-afe7-031f28703328",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+-----------+\n|database|       tableName|isTemporary|\n+--------+----------------+-----------+\n| default|     name_basics|      false|\n| default|      title_akas|      false|\n| default|    title_basics|      false|\n| default|      title_crew|      false|\n| default|   title_episode|      false|\n| default|title_principals|      false|\n| default|   title_ratings|      false|\n+--------+----------------+-----------+\n\n=========================================\n📋 Schema simplificado da tabela: name_basics\n=========================================\nroot\n |-- nconst: string (nullable = true)\n |-- primaryName: string (nullable = true)\n |-- birthYear: string (nullable = true)\n |-- deathYear: string (nullable = true)\n |-- primaryProfession: string (nullable = true)\n |-- knownForTitles: string (nullable = true)\n\n\n\n\n=========================================\n📋 Schema simplificado da tabela: title_akas\n=========================================\nroot\n |-- titleId: string (nullable = true)\n |-- ordering: integer (nullable = true)\n |-- title: string (nullable = true)\n |-- region: string (nullable = true)\n |-- language: string (nullable = true)\n |-- types: string (nullable = true)\n |-- attributes: string (nullable = true)\n |-- isOriginalTitle: integer (nullable = true)\n\n\n\n\n=========================================\n📋 Schema simplificado da tabela: title_basics\n=========================================\nroot\n |-- tconst: string (nullable = true)\n |-- titleType: string (nullable = true)\n |-- primaryTitle: string (nullable = true)\n |-- originalTitle: string (nullable = true)\n |-- isAdult: string (nullable = true)\n |-- startYear: string (nullable = true)\n |-- endYear: string (nullable = true)\n |-- runtimeMinutes: string (nullable = true)\n |-- genres: string (nullable = true)\n\n\n\n\n=========================================\n📋 Schema simplificado da tabela: title_crew\n=========================================\nroot\n |-- tconst: string (nullable = true)\n |-- directors: string (nullable = true)\n |-- writers: string (nullable = true)\n\n\n\n\n=========================================\n📋 Schema simplificado da tabela: title_episode\n=========================================\nroot\n |-- tconst: string (nullable = true)\n |-- parentTconst: string (nullable = true)\n |-- seasonNumber: string (nullable = true)\n |-- episodeNumber: string (nullable = true)\n\n\n\n\n=========================================\n📋 Schema simplificado da tabela: title_ratings\n=========================================\nroot\n |-- tconst: string (nullable = true)\n |-- averageRating: double (nullable = true)\n |-- numVotes: integer (nullable = true)\n\n\n\n\n=========================================\n📋 Schema simplificado da tabela: title_principals\n=========================================\nroot\n |-- tconst: string (nullable = true)\n |-- ordering: integer (nullable = true)\n |-- nconst: string (nullable = true)\n |-- category: string (nullable = true)\n |-- job: string (nullable = true)\n |-- characters: string (nullable = true)\n\n\n\n\n"
     ]
    }
   ],
   "source": [
    "# Verificação de todas as tabelas existentes no metastore\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "# Verificação dos schemas\n",
    "tables = [\n",
    "    \"name_basics\", \n",
    "    \"title_akas\", \n",
    "    \"title_basics\", \n",
    "    \"title_crew\", \n",
    "    \"title_episode\", \n",
    "    \"title_ratings\",\n",
    "    \"title_principals\"\n",
    "]\n",
    "\n",
    "for table in tables:\n",
    "    print(\"=========================================\")\n",
    "    print(f\"📋 Schema simplificado da tabela: {table}\")\n",
    "    print(\"=========================================\")\n",
    "    spark.table(table).printSchema()\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c8b0574-0522-446d-a758-7687da265549",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusão\n",
    "\n",
    "Finalizada a etapa de extração e upload dos dados.\n",
    "\n",
    "Nos próximos notebooks, vamos realizar a preparação dos dados na camada silver (notebook3) e realizar consultas analíticas com os dados já trabalhados, na camada gold (notebook4).\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 370416811279879,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Notebook2 (Camada Bronze) - Extração e Upload dos dados",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}